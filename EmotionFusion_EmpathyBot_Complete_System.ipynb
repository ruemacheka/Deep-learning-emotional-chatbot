{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOctW+0j+zQtTON7NcEl/mM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruemacheka/Deep-learning-emotional-chatbot/blob/main/EmotionFusion_EmpathyBot_Complete_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "EmotionFusion EmpathyBot - THE COMPLETE IMPLEMENTATION (100%)\n"
      ],
      "metadata": {
        "id": "Vf7TwKtFe1ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation of all packages"
      ],
      "metadata": {
        "id": "phjaI7MSe5Kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q transformers datasets torch torchvision torchaudio\n",
        "!pip install -q fer==22.4.0 mtcnn opencv-python-headless\n",
        "!pip install -q gradio librosa soundfile\n",
        "!pip install -q scikit-learn matplotlib seaborn\n",
        "!pip install -q accelerate\n",
        "\n",
        "print(\"âœ… Installation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOKRPetFbawt",
        "outputId": "9738f6f3-d546-4d13-c895-87f61d386d51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Installation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2"
      ],
      "metadata": {
        "id": "nozm8tZ1bie2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from fer import FER\n",
        "import cv2\n",
        "import librosa\n",
        "import gradio as gr\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCcf67zYblNO",
        "outputId": "98a30d9e-8b7d-41a1-9cbb-8407fa3b1ad9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”§ Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Emotion Model"
      ],
      "metadata": {
        "id": "60RXlCUPdLwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEmotionModel:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
        "        self.emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
        "        self.model.to(device)\n",
        "        self.pipeline = pipeline(\"text-classification\", model=self.model, tokenizer=self.tokenizer,\n",
        "                                return_all_scores=True, device=0 if torch.cuda.is_available() else -1)\n",
        "        print(\"âœ… Text emotion model loaded\")\n",
        "\n",
        "    def predict(self, text):\n",
        "        if not text or not text.strip():\n",
        "            return {e: 0.14 for e in ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']}\n",
        "        try:\n",
        "            results = self.pipeline(text)[0]\n",
        "            probs = {item['label'].lower(): float(item['score']) for item in results}\n",
        "            mapping = {'anger': 'angry', 'joy': 'happy', 'sadness': 'sad'}\n",
        "            normalized = {mapping.get(k, k): v for k, v in probs.items()}\n",
        "            return normalized\n",
        "        except:\n",
        "            return {e: 0.14 for e in ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']}"
      ],
      "metadata": {
        "id": "gKnh8qHubsZE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Face Emotion Model"
      ],
      "metadata": {
        "id": "7pJyoEvndSj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomEmotionCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(CustomEmotionCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64),\n",
        "                                   nn.ReLU(), nn.MaxPool2d(2, 2), nn.Dropout(0.25))\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128),\n",
        "                                   nn.ReLU(), nn.MaxPool2d(2, 2), nn.Dropout(0.25))\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256),\n",
        "                                   nn.ReLU(), nn.MaxPool2d(2, 2), nn.Dropout(0.25))\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512),\n",
        "                                   nn.ReLU(), nn.MaxPool2d(2, 2), nn.Dropout(0.25))\n",
        "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(512 * 3 * 3, 1024), nn.BatchNorm1d(1024),\n",
        "                               nn.ReLU(), nn.Dropout(0.5), nn.Linear(1024, 512), nn.BatchNorm1d(512),\n",
        "                               nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "class FaceEmotionModel:\n",
        "    def __init__(self):\n",
        "        self.emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "        self.fer_detector = FER(mtcnn=True)\n",
        "        self.custom_cnn = CustomEmotionCNN(num_classes=7).to(device)\n",
        "        print(\"âœ… Face emotion model loaded\")\n",
        "\n",
        "    def predict(self, image):\n",
        "        if image is None or image.size == 0:\n",
        "            return {label: 0.14 for label in self.emotion_labels}\n",
        "        try:\n",
        "            if len(image.shape) == 2:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "            elif image.shape[2] == 4:\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
        "            results = self.fer_detector.detect_emotions(image)\n",
        "            if not results:\n",
        "                return {label: 0.14 for label in self.emotion_labels}\n",
        "            emotions = results[0]['emotions']\n",
        "            return {k.lower(): float(v) for k, v in emotions.items()}\n",
        "        except:\n",
        "            return {label: 0.14 for label in self.emotion_labels}"
      ],
      "metadata": {
        "id": "pkgOjBp3cF-f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voice Emotion Model"
      ],
      "metadata": {
        "id": "li_RZttcdbRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VoiceEmotionModel:\n",
        "    def __init__(self):\n",
        "        self.emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "        self.sample_rate = 16000\n",
        "        print(\"âœ… Voice emotion model initialized\")\n",
        "\n",
        "    def extract_features(self, audio, sr):\n",
        "        try:\n",
        "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
        "            mfccs_mean = np.mean(mfccs, axis=1)\n",
        "            pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)\n",
        "            pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0\n",
        "            energy = np.sum(librosa.feature.rms(y=audio))\n",
        "            zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "            spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
        "            spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))\n",
        "            return {'pitch': pitch_mean, 'energy': energy, 'zcr': zcr,\n",
        "                   'spectral_centroid': spectral_centroid, 'spectral_rolloff': spectral_rolloff}\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def predict(self, audio_path=None, audio_array=None, sr=None):\n",
        "        try:\n",
        "            if audio_path:\n",
        "                audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
        "            elif audio_array is not None and sr is not None:\n",
        "                audio = audio_array\n",
        "            else:\n",
        "                return {label: 0.14 for label in self.emotion_labels}\n",
        "\n",
        "            features = self.extract_features(audio, sr)\n",
        "            if features is None:\n",
        "                return {label: 0.14 for label in self.emotion_labels}\n",
        "\n",
        "            probs = {label: 0.0 for label in self.emotion_labels}\n",
        "            pitch = features['pitch']\n",
        "            energy = features['energy']\n",
        "\n",
        "            if energy > 100 and pitch > 150:\n",
        "                probs = {'happy': 0.4, 'surprise': 0.3, 'neutral': 0.3}\n",
        "            elif energy > 100 and pitch < 150:\n",
        "                probs = {'angry': 0.5, 'fear': 0.2, 'neutral': 0.3}\n",
        "            elif energy < 50:\n",
        "                probs = {'sad': 0.4, 'neutral': 0.4, 'fear': 0.2}\n",
        "            else:\n",
        "                probs = {'neutral': 0.7, 'happy': 0.15, 'sad': 0.15}\n",
        "\n",
        "            total = sum(probs.values())\n",
        "            return {k: v/total for k, v in probs.items()}\n",
        "        except:\n",
        "            return {label: 0.14 for label in self.emotion_labels}"
      ],
      "metadata": {
        "id": "Q6SsTWjycNhf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion Model"
      ],
      "metadata": {
        "id": "xISZeODEdkjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionFusion(nn.Module):\n",
        "    def __init__(self, input_dim=7, num_modalities=3, hidden_dim=64):\n",
        "        super(AttentionFusion, self).__init__()\n",
        "        self.num_modalities = num_modalities\n",
        "        self.text_encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.face_encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.voice_encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.3))\n",
        "        self.attention = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), nn.Tanh(),\n",
        "                                      nn.Linear(hidden_dim // 2, 1))\n",
        "        self.classifier = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(),\n",
        "                                       nn.Dropout(0.3), nn.Linear(hidden_dim // 2, input_dim), nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, text_probs, face_probs, voice_probs):\n",
        "        text_feat = self.text_encoder(text_probs)\n",
        "        face_feat = self.face_encoder(face_probs)\n",
        "        voice_feat = self.voice_encoder(voice_probs)\n",
        "        features = torch.stack([text_feat, face_feat, voice_feat], dim=1)\n",
        "        attn_weights = torch.softmax(self.attention(features), dim=1)\n",
        "        weighted_features = (features * attn_weights).sum(dim=1)\n",
        "        output = self.classifier(weighted_features)\n",
        "        return output, attn_weights.squeeze(-1)\n",
        "\n",
        "class MultimodalFusionModel:\n",
        "    def __init__(self):\n",
        "        self.emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "        self.fusion_net = AttentionFusion(input_dim=7, num_modalities=3).to(device)\n",
        "        print(\"âœ… Multimodal fusion model initialized\")\n",
        "\n",
        "    def predict(self, text_probs, face_probs, voice_probs):\n",
        "        try:\n",
        "            text_tensor = torch.FloatTensor([[text_probs.get(e, 0.0) for e in self.emotion_labels]]).to(device)\n",
        "            face_tensor = torch.FloatTensor([[face_probs.get(e, 0.0) for e in self.emotion_labels]]).to(device)\n",
        "            voice_tensor = torch.FloatTensor([[voice_probs.get(e, 0.0) for e in self.emotion_labels]]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fused_probs, attention_weights = self.fusion_net(text_tensor, face_tensor, voice_tensor)\n",
        "\n",
        "            fused_dict = {self.emotion_labels[i]: float(fused_probs[0, i]) for i in range(len(self.emotion_labels))}\n",
        "            attention_dict = {'text': float(attention_weights[0, 0]), 'face': float(attention_weights[0, 1]),\n",
        "                            'voice': float(attention_weights[0, 2])}\n",
        "            return fused_dict, attention_dict\n",
        "        except:\n",
        "            fused = {}\n",
        "            for emotion in self.emotion_labels:\n",
        "                fused[emotion] = (text_probs.get(emotion, 0.0) + face_probs.get(emotion, 0.0) +\n",
        "                                voice_probs.get(emotion, 0.0)) / 3.0\n",
        "            total = sum(fused.values())\n",
        "            if total > 0:\n",
        "                fused = {k: v/total for k, v in fused.items()}\n",
        "            return fused, {'text': 0.33, 'face': 0.33, 'voice': 0.33}"
      ],
      "metadata": {
        "id": "W37MJFH8cV2C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empathy Response System"
      ],
      "metadata": {
        "id": "wZg-C2iJdsyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EmpathyResponseSystem:\n",
        "    def __init__(self):\n",
        "        self.responses = {\n",
        "            'happy': [\"That's wonderful! I'm so glad to hear that. What made your day special?\",\n",
        "                     \"Your happiness is contagious! Tell me more about what's bringing you joy.\"],\n",
        "            'sad': [\"I'm sorry you're feeling down. I'm here to listen if you want to talk.\",\n",
        "                   \"It's okay to feel sad sometimes. Would you like to share what's on your mind?\"],\n",
        "            'angry': [\"That sounds really frustrating. Would you like to talk through what's bothering you?\",\n",
        "                     \"I can sense your frustration. Sometimes it helps to express what's making you angry.\"],\n",
        "            'fear': [\"It's completely normal to feel worried or anxious. You're not alone.\",\n",
        "                    \"I hear that you're feeling scared. Would you like to talk about what's concerning you?\"],\n",
        "            'surprise': [\"That must have been unexpected! How are you processing this?\",\n",
        "                        \"Surprises can be quite overwhelming. Tell me how you're feeling about this.\"],\n",
        "            'disgust': [\"That sounds unpleasant. I'm here if you need to talk about it.\",\n",
        "                       \"I understand that must be bothering you. Would you like to share more?\"],\n",
        "            'neutral': [\"Thanks for sharing. How are you feeling today?\",\n",
        "                       \"I'm here to listen. Is there anything on your mind?\"]\n",
        "        }\n",
        "        self.conflict_responses = [\n",
        "            \"I'm sensing some mixed emotions here. Would you like to talk about it?\",\n",
        "            \"It seems like there might be complexity to what you're feeling. I'm here to help.\"\n",
        "        ]\n",
        "\n",
        "    def generate(self, emotion, confidence):\n",
        "        import random\n",
        "        if emotion in self.responses:\n",
        "            response = random.choice(self.responses[emotion])\n",
        "            if confidence < 0.5:\n",
        "                response = \"I'm not entirely certain, but \" + response[0].lower() + response[1:]\n",
        "            return response\n",
        "        return \"I'm here to listen. How are you feeling?\""
      ],
      "metadata": {
        "id": "jNkFoO6ucdpb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialization of All Models\n"
      ],
      "metadata": {
        "id": "TTZj8ZO0dzrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸš€ Initializing EmotionFusion EmpathyBot...\")\n",
        "text_model = TextEmotionModel()\n",
        "face_model = FaceEmotionModel()\n",
        "voice_model = VoiceEmotionModel()\n",
        "fusion_model = MultimodalFusionModel()\n",
        "empathy_system = EmpathyResponseSystem()\n",
        "print(\"\\nâœ… All systems initialized successfully!\")\n",
        "print(\"\\nðŸ“Š System Capabilities:\")\n",
        "print(\"  â€¢ Text Emotion Recognition (DistilRoBERTa)\")\n",
        "print(\"  â€¢ Facial Emotion Recognition (FER + Custom CNN)\")\n",
        "print(\"  â€¢ Voice Emotion Recognition (Audio Features)\")\n",
        "print(\"  â€¢ Learned Multimodal Fusion (Attention Mechanism)\")\n",
        "print(\"  â€¢ Empathetic Response Generation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezUjojhBclgC",
        "outputId": "e3a70133-7ed2-4c78-b6fe-d4270198ff81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Initializing EmotionFusion EmpathyBot...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Text emotion model loaded\n",
            "âœ… Face emotion model loaded\n",
            "âœ… Voice emotion model initialized\n",
            "âœ… Multimodal fusion model initialized\n",
            "\n",
            "âœ… All systems initialized successfully!\n",
            "\n",
            "ðŸ“Š System Capabilities:\n",
            "  â€¢ Text Emotion Recognition (DistilRoBERTa)\n",
            "  â€¢ Facial Emotion Recognition (FER + Custom CNN)\n",
            "  â€¢ Voice Emotion Recognition (Audio Features)\n",
            "  â€¢ Learned Multimodal Fusion (Attention Mechanism)\n",
            "  â€¢ Empathetic Response Generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Processing Function"
      ],
      "metadata": {
        "id": "7ouleL1SeJlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_multimodal_input(text, image, audio):\n",
        "    results = {}\n",
        "\n",
        "    try:\n",
        "        # Text\n",
        "        if text and text.strip():\n",
        "            results['text_probs'] = text_model.predict(text)\n",
        "        else:\n",
        "            results['text_probs'] = {e: 0.14 for e in fusion_model.emotion_labels}\n",
        "\n",
        "        # Face\n",
        "        if image is not None:\n",
        "            results['face_probs'] = face_model.predict(image)\n",
        "        else:\n",
        "            results['face_probs'] = {e: 0.14 for e in fusion_model.emotion_labels}\n",
        "\n",
        "        # Voice\n",
        "        if audio is not None:\n",
        "            sr, audio_array = audio\n",
        "            results['voice_probs'] = voice_model.predict(audio_array=audio_array, sr=sr)\n",
        "        else:\n",
        "            results['voice_probs'] = {e: 0.14 for e in fusion_model.emotion_labels}\n",
        "\n",
        "        # Fusion\n",
        "        fused_probs, attention_weights = fusion_model.predict(\n",
        "            results['text_probs'], results['face_probs'], results['voice_probs'])\n",
        "\n",
        "        results['fused_probs'] = fused_probs\n",
        "        results['attention_weights'] = attention_weights\n",
        "        results['final_emotion'] = max(fused_probs.items(), key=lambda x: x[1])[0]\n",
        "        results['confidence'] = fused_probs[results['final_emotion']]\n",
        "\n",
        "        # Generate response\n",
        "        results['empathy_response'] = empathy_system.generate(\n",
        "            results['final_emotion'], results['confidence'])\n",
        "\n",
        "        # Format output\n",
        "        output = f\"\"\"\n",
        "**ðŸŽ¯ FINAL EMOTION: {results['final_emotion'].upper()}**\n",
        "**Confidence: {results['confidence']:.2%}**\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“Š Individual Modality Predictions:\n",
        "\n",
        "**ðŸ’¬ Text Analysis:**\n",
        "Top emotion: {max(results['text_probs'].items(), key=lambda x: x[1])[0].capitalize()} ({max(results['text_probs'].values()):.2%})\n",
        "\n",
        "**ðŸ‘¤ Facial Expression:**\n",
        "Top emotion: {max(results['face_probs'].items(), key=lambda x: x[1])[0].capitalize()} ({max(results['face_probs'].values()):.2%})\n",
        "\n",
        "**ðŸŽ¤ Voice Analysis:**\n",
        "Top emotion: {max(results['voice_probs'].items(), key=lambda x: x[1])[0].capitalize()} ({max(results['voice_probs'].values()):.2%})\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Attention Weights:\n",
        "- Text: {results['attention_weights']['text']:.2%}\n",
        "- Face: {results['attention_weights']['face']:.2%}\n",
        "- Voice: {results['attention_weights']['voice']:.2%}\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’™ Empathetic Response:\n",
        "\n",
        "{results['empathy_response']}\n",
        "\"\"\"\n",
        "        return output\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing input: {str(e)}\""
      ],
      "metadata": {
        "id": "HCD_m_cjc3ob"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation and Launching of the Gradio Interface"
      ],
      "metadata": {
        "id": "C5Ezuo0feSkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "F7YDOxDyXrF-",
        "outputId": "4a90a1e0-aab5-4c0d-825a-abd81350c8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸŽ‰ EMOTIONFUSION EMPATHYBOT IS READY!\n",
            "======================================================================\n",
            "\n",
            "Launching interface...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://c96fd41478dcf34c29.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c96fd41478dcf34c29.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "interface = gr.Interface(\n",
        "    fn=process_multimodal_input,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=3, placeholder=\"Type how you're feeling...\", label=\"ðŸ’¬ Text Input\"),\n",
        "        gr.Image(label=\"ðŸ‘¤ Face Image (Upload a photo)\", type=\"numpy\"),\n",
        "        gr.Audio(label=\"ðŸŽ¤ Voice Recording\", type=\"numpy\")\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Analysis Results\"),\n",
        "    title=\"ðŸ¤– EmotionFusion EmpathyBot - Complete System\",\n",
        "    description=\"\"\"\n",
        "    **Trimodal Emotion Recognition & Empathetic AI Assistant**\n",
        "\n",
        "    Analyzes emotions from:\n",
        "    - ðŸ’¬ **Text**: What you write\n",
        "    - ðŸ‘¤ **Face**: Your facial expression\n",
        "    - ðŸŽ¤ **Voice**: Your tone and speech\n",
        "\n",
        "    Uses attention mechanisms to intelligently combine all three modalities!\n",
        "    \"\"\",\n",
        "    examples=[\n",
        "        [\"I'm so excited about my new job!\", None, None],\n",
        "        [\"I'm feeling really overwhelmed today...\", None, None]\n",
        "    ],\n",
        "    theme=gr.themes.Soft()\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸŽ‰ EMOTIONFUSION EMPATHYBOT IS READY!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nLaunching interface...\")\n",
        "\n",
        "# Launch with share=True to get public URL\n",
        "interface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9RoERYQZkrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}